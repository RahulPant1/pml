---
title: "Practical Machine Learning:Project Submission"
author: "Rahul"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Training and Test Data Location

Training data location https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data location https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Loading the required libraries

```{r, message=FALSE}
set.seed(233)
library("caret")
library(ggplot2)
library(dplyr)
library("funModeling")
library(randomForest)
```

## Data Loading and Data Partition

The training and test data is loaded in the environment. We now partition the training data into 'subTrain' and 'subTest' set in the ratio of 75:25 respectively. The 'subTrain' wil be used for model traingin and 'subTest' for accuracy testing purpose.


```{r}

training<-read.csv("~/Coursera/pml-training.csv",stringsAsFactors = FALSE)
training$classe<-as.factor(training$classe)
testing<-read.csv("~/Coursera/pml-testing.csv",stringsAsFactors = FALSE)


inTrain = createDataPartition(training$classe, p = 3/4)[[1]]
subTrain = training[ inTrain,]
subTest = training[-inTrain,]

```

## Data Cleaning
1. We identify cloumns which have more than 50% entries as NULL's 
2. Identify columns which have near zero variance. 
These columns may not add any value in the analysis and hence are removed from the training dataset 

3. Additionally few more columns are removed(first 7 columns) namely
   Timestamp related
   user name 
   index(X)
   Window related


```{r}


# Removed all columns having more than 50% NA's
dummy<-subTrain[, -which(colMeans(is.na(subTrain)) > 0.5)]

# Removed few columns as mentioned above
dummy<-dummy[,-c(1:7)]

# Removed all columns having very less variance
dummy<-dummy[,-nearZeroVar(dummy)]

```

## Model Training

RandomForest was used for evaluation. Experimented with cross-validation in random forest too, but test accuracy did not change significantly with or without "cv", I went ahead without cross-validation. (This may be expected as random forest split the data in 70:30 during run-time as well as randomize variable selction, hence providing inherent cross-validation.) 

```{r}

rfmod<-randomForest(classe ~ ., data = dummy, importance = TRUE, ntrees = 10)

```

## Model Evaluation
Model Evaluation was done on 'subTest' data and the above model with 10 trees, gave 99.6% accuracy.

```{r}

mx<-confusionMatrix(subTest$classe,predict(rfmod,subTest))
mx$overall["Accuracy"]
```


## Variable Importance Plot
```{r}

varImpPlot(rfmod,type=2)
```

